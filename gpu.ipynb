{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koushika894/Character-Recognization-GPU/blob/main/gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF0fkzMoE0OM",
        "outputId": "361bfc34-b094-4219-fd59-5dcbffdbdba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2024.1.2.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.7 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2024.1.18-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (4.3.6)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (3.0.2)\n",
            "Downloading pytools-2024.1.18-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.8/89.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2024.1.2-cp310-cp310-linux_x86_64.whl size=660545 sha256=f7a0fd012778e58cc9f5f90842b72c79778e18c1775d24e472eac8de6b6c3c9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/63/40/4bf006182f942d3516b71bb2ff3b57ccbdb8b2c0ee81882b6e\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.3.6 pycuda-2024.1.2 pytools-2024.1.18\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import pycuda.driver as drv\n",
        "from pycuda.compiler import SourceModule\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pycuda.gpuarray as gpuarray\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import time"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "p4Ux5cp9-i3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gT4kUjv15XjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_data = np.array([\n",
        "    [[1, 0, 0, 0, 1],\n",
        "     [1, 1, 0, 0, 1],\n",
        "     [1, 0, 1, 0, 1],\n",
        "     [1, 0, 0, 1, 1],\n",
        "     [1, 0, 0, 0, 1]],\n",
        "\n",
        "    [[1, 0, 0, 0, 1],\n",
        "     [1, 1, 0, 0, 1],\n",
        "     [1, 0, 1, 0, 1],\n",
        "     [1, 0, 0, 1, 1],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [1, 1, 0, 0, 1],\n",
        "     [1, 0, 1, 0, 1],\n",
        "     [1, 0, 0, 1, 1],\n",
        "     [1, 0, 0, 0, 1]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [1, 1, 0, 0, 1],\n",
        "     [1, 0, 1, 0, 1],\n",
        "     [1, 0, 0, 1, 1],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[1, 0, 0, 1, 0],\n",
        "     [1, 1, 0, 1, 0],\n",
        "     [1, 0, 1, 1, 0],\n",
        "     [0, 0, 0, 0, 0],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [0, 0, 0, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 1, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0]],\n",
        "\n",
        "    [[1, 0, 0, 0, 1],\n",
        "     [1, 1, 0, 0, 1],\n",
        "     [1, 0, 1, 0, 1],\n",
        "     [1, 0, 0, 1, 1],\n",
        "     [1, 0, 0, 0, 1]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [0, 0, 0, 0, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [0, 1, 0, 1, 0]],\n",
        "\n",
        "    [[1, 0, 1, 0, 0],\n",
        "     [1, 1, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [0, 0, 0, 0, 0],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [0, 1, 0, 0, 0],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 1, 1],\n",
        "     [0, 0, 1, 0, 1]]\n",
        "])\n"
      ],
      "metadata": {
        "id": "BulNnQ9R9_zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A_data = np.array([\n",
        "    [[0, 0, 1, 0, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [0, 1, 0, 1, 0],\n",
        "     [0, 1, 0, 1, 0]],\n",
        "\n",
        "    [[0, 0, 1, 0, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [0, 0, 0, 0, 0],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [0, 0, 0, 0, 0],\n",
        "     [0, 0, 1, 0, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1]],\n",
        "\n",
        "    [[0, 1, 0, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 1, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 1, 0],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 1, 1],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 0, 1]],\n",
        "\n",
        "    [[0, 0, 1, 1, 0],\n",
        "     [0, 1, 0, 0, 1],\n",
        "     [0, 1, 1, 1, 1],\n",
        "     [0, 1, 0, 0, 1],\n",
        "     [0, 1, 0, 0, 1]],\n",
        "\n",
        "    [[0, 1, 1, 0, 0],\n",
        "     [1, 0, 0, 1, 0],\n",
        "     [1, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 1, 0],\n",
        "     [1, 0, 0, 1, 0]],\n",
        "\n",
        "    [[0, 1, 0, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 1, 1, 0, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 1, 0],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 1, 1, 1],\n",
        "     [0, 0, 1, 0, 1],\n",
        "     [0, 0, 0, 0, 0]],\n",
        "\n",
        "    [[0, 0, 0, 0, 0],\n",
        "     [0, 0, 1, 0, 0],\n",
        "     [0, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [0, 0, 0, 0, 0]]\n",
        "])"
      ],
      "metadata": {
        "id": "iJcFi1ky5f8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R_data = np.array([\n",
        "    [[1, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [1, 1, 1, 1, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 0, 1, 1]],\n",
        "\n",
        "    [[1, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [1, 1, 1, 1, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 0, 1, 1]],\n",
        "\n",
        "    [[1, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [1, 1, 1, 1, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 0, 1, 1]],\n",
        "\n",
        "    [[1, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [1, 1, 1, 1, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 0, 1, 1]],\n",
        "\n",
        "    [[1, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [1, 1, 1, 1, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 0, 1, 1]],\n",
        "\n",
        "    [[1, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [1, 1, 1, 1, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 0, 1, 1]],\n",
        "\n",
        "    [[1, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [1, 1, 1, 1, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 0, 1, 1]],\n",
        "\n",
        "    [[1, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [1, 1, 1, 1, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 0, 1, 1]],\n",
        "\n",
        "    [[1, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [1, 1, 1, 1, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 0, 1, 1]],\n",
        "\n",
        "    [[1, 1, 1, 1, 0],\n",
        "     [1, 0, 0, 0, 1],\n",
        "     [1, 1, 1, 1, 0],\n",
        "     [1, 0, 1, 0, 0],\n",
        "     [1, 0, 0, 1, 1]]\n",
        "])\n"
      ],
      "metadata": {
        "id": "7KLZJ5sJ5kNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_data = N_data.reshape(10,25)\n",
        "A_data = A_data.reshape(10,25)\n",
        "R_data = R_data.reshape(10,25)"
      ],
      "metadata": {
        "id": "fgMMImU06Bd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "labels = []\n",
        "for k,i in enumerate([N_data,A_data,R_data]):\n",
        "    dataset.append(i)\n",
        "    for j in range(10):\n",
        "        labels.append(k)\n",
        "dataset = np.concatenate(dataset,axis = 0)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "xwaa8E2i6N3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "id": "hlEfNlJw6UN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a44d5fc-63a9-4f07-de07-9ee4b52aee91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(dataset,labels,train_size = 0.7)"
      ],
      "metadata": {
        "id": "k9R0bfKl68HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 21\n",
        "input_size = 25\n",
        "hidden_size = 10\n",
        "output_size = 3\n",
        "learning_rate = 0.01\n",
        "\n",
        "w1 = np.random.randn(input_size, hidden_size).astype(np.float32)\n",
        "w2 = np.random.randn(hidden_size, output_size).astype(np.float32)\n",
        "w1_gpu = drv.mem_alloc(w1.nbytes)\n",
        "w2_gpu = drv.mem_alloc(w2.nbytes)\n",
        "drv.memcpy_htod(w1_gpu, w1)\n",
        "drv.memcpy_htod(w2_gpu, w2)"
      ],
      "metadata": {
        "id": "TIRXu3lF69wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod = SourceModule(\"\"\"\n",
        "__global__ void forward(float *input, float *w1, float *w2, float *z1, float *z2,\n",
        "                       int input_size, int hidden_size, int output_size, int batch_size) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    int hidden_idx = threadIdx.x;\n",
        "\n",
        "    if (hidden_idx < hidden_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < input_size; i++) {\n",
        "            sum += input[batch_idx * input_size + i] * w1[i * hidden_size + hidden_idx];\n",
        "        }\n",
        "        z1[batch_idx * hidden_size + hidden_idx] = fmaxf(0.0f, sum);\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    int output_idx = threadIdx.x;\n",
        "    if (output_idx < output_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int j = 0; j < hidden_size; j++) {\n",
        "            sum += z1[batch_idx * hidden_size + j] * w2[j * output_size + output_idx];\n",
        "        }\n",
        "        z2[batch_idx * output_size + output_idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void backprop(float *input, float *z1, float *z2, float *target, float *w1, float *w2,\n",
        "                        int input_size, int hidden_size, int output_size, int batch_size, float learning_rate) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    int output_idx = threadIdx.x;\n",
        "\n",
        "    if (output_idx < output_size) {\n",
        "        float delta2 = z2[batch_idx * output_size + output_idx] - target[batch_idx * output_size + output_idx];\n",
        "        for (int j = 0; j < hidden_size; j++) {\n",
        "            atomicAdd(&w2[j * output_size + output_idx],\n",
        "                     -learning_rate * delta2 * z1[batch_idx * hidden_size + j]);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int hidden_idx = threadIdx.x;\n",
        "    if (hidden_idx < hidden_size) {\n",
        "        float delta1 = 0.0f;\n",
        "        for (int k = 0; k < output_size; k++) {\n",
        "            delta1 += (z2[batch_idx * output_size + k] - target[batch_idx * output_size + k])\n",
        "                     * w2[hidden_idx * output_size + k];\n",
        "        }\n",
        "        delta1 *= (z1[batch_idx * hidden_size + hidden_idx] > 0.0f) ? 1.0f : 0.0f;\n",
        "\n",
        "        for (int i = 0; i < input_size; i++) {\n",
        "            atomicAdd(&w1[i * hidden_size + hidden_idx],\n",
        "                     -learning_rate * delta1 * input[batch_idx * input_size + i]);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "forward_kernel = mod.get_function(\"forward\")\n",
        "backprop_kernel = mod.get_function(\"backprop\")"
      ],
      "metadata": {
        "id": "Zx8iUXN17B7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ANN:\n",
        "    def __init__(self, input_size=25, hidden_size=64, output_size=3, learning_rate=0.01):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.w1 = np.random.randn(input_size, hidden_size).astype(np.float32) * np.sqrt(2.0 / input_size)\n",
        "        self.w2 = np.random.randn(hidden_size, output_size).astype(np.float32) * np.sqrt(2.0 / hidden_size)\n",
        "\n",
        "        self.forward_kernel = mod.get_function(\"forward\")\n",
        "        self.backprop_kernel = mod.get_function(\"backprop\")\n",
        "\n",
        "        self.w1_gpu = gpuarray.to_gpu(self.w1)\n",
        "        self.w2_gpu = gpuarray.to_gpu(self.w2)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def train_batch(self, batch_x, batch_y):\n",
        "        batch_size = len(batch_x)\n",
        "\n",
        "        inputs_gpu = gpuarray.to_gpu(batch_x.astype(np.float32))\n",
        "        targets_gpu = gpuarray.to_gpu(batch_y.astype(np.float32))\n",
        "        z1_gpu = gpuarray.zeros((batch_size, self.hidden_size), dtype=np.float32)\n",
        "        z2_gpu = gpuarray.zeros((batch_size, self.output_size), dtype=np.float32)\n",
        "\n",
        "        block_dim = (max(self.hidden_size, self.output_size), 1, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        self.forward_kernel(inputs_gpu, self.w1_gpu, self.w2_gpu, z1_gpu, z2_gpu,\n",
        "                          np.int32(self.input_size), np.int32(self.hidden_size),\n",
        "                          np.int32(self.output_size), np.int32(batch_size),\n",
        "                          block=block_dim, grid=grid_dim)\n",
        "\n",
        "        outputs = self.softmax(z2_gpu.get())\n",
        "        z2_gpu.gpudata.free()\n",
        "        z2_gpu = gpuarray.to_gpu(outputs.astype(np.float32))\n",
        "\n",
        "        self.backprop_kernel(inputs_gpu, z1_gpu, z2_gpu, targets_gpu,\n",
        "                           self.w1_gpu, self.w2_gpu,\n",
        "                           np.int32(self.input_size), np.int32(self.hidden_size),\n",
        "                           np.int32(self.output_size), np.int32(batch_size),\n",
        "                           np.float32(self.learning_rate),\n",
        "                           block=block_dim, grid=grid_dim)\n",
        "\n",
        "        inputs_gpu.gpudata.free()\n",
        "        targets_gpu.gpudata.free()\n",
        "        z1_gpu.gpudata.free()\n",
        "        z2_gpu.gpudata.free()\n",
        "\n",
        "    def predict(self, X):\n",
        "        batch_size = len(X)\n",
        "\n",
        "        inputs_gpu = gpuarray.to_gpu(X.astype(np.float32))\n",
        "        z1_gpu = gpuarray.zeros((batch_size, self.hidden_size), dtype=np.float32)\n",
        "        z2_gpu = gpuarray.zeros((batch_size, self.output_size), dtype=np.float32)\n",
        "\n",
        "        block_dim = (max(self.hidden_size, self.output_size), 1, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        self.forward_kernel(inputs_gpu, self.w1_gpu, self.w2_gpu, z1_gpu, z2_gpu,\n",
        "                          np.int32(self.input_size), np.int32(self.hidden_size),\n",
        "                          np.int32(self.output_size), np.int32(batch_size),\n",
        "                          block=block_dim, grid=grid_dim)\n",
        "\n",
        "        outputs = self.softmax(z2_gpu.get())\n",
        "\n",
        "        inputs_gpu.gpudata.free()\n",
        "        z1_gpu.gpudata.free()\n",
        "        z2_gpu.gpudata.free()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, epochs=100, batch_size=32):\n",
        "    net = ANN(input_size=25, hidden_size=10, output_size=3)\n",
        "\n",
        "    y_train_onehot = np.zeros((len(y_train), 3))\n",
        "    y_train_onehot[np.arange(len(y_train)), y_train] = 1\n",
        "\n",
        "    n_batches = len(X_train) // batch_size\n",
        "    print(\"Training started...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(len(X_train))\n",
        "        X_train_shuffled = X_train[indices]\n",
        "        y_train_shuffled = y_train_onehot[indices]\n",
        "\n",
        "        for batch in range(n_batches):\n",
        "            start_idx = batch * batch_size\n",
        "            end_idx = start_idx + batch_size\n",
        "            batch_x = X_train_shuffled[start_idx:end_idx]\n",
        "            batch_y = y_train_shuffled[start_idx:end_idx]\n",
        "            net.train_batch(batch_x, batch_y)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            train_preds = np.argmax(net.predict(X_train), axis=1)\n",
        "            train_acc = accuracy_score(y_train, train_preds)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Training Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "    test_preds = np.argmax(net.predict(X_test), axis=1)\n",
        "    test_acc = accuracy_score(y_test, test_preds)\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "3FkX6gv_7DQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_and_evaluate(X_train, y_train, X_test, y_test, epochs=50, batch_size=21)"
      ],
      "metadata": {
        "id": "kVtKStfg7JaD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "050f3688-2aba-4f82-a9dc-63c7a3b50378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started...\n",
            "Epoch 10/50, Training Accuracy: 0.9524\n",
            "Epoch 20/50, Training Accuracy: 0.9524\n",
            "Epoch 30/50, Training Accuracy: 1.0000\n",
            "Epoch 40/50, Training Accuracy: 1.0000\n",
            "Epoch 50/50, Training Accuracy: 1.0000\n",
            "\n",
            "Test Results:\n",
            "Test Accuracy: 0.8889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod = SourceModule(\"\"\"\n",
        "__global__ void conv2d(float *input, float *filters, float *output,\n",
        "                      int batch_size, int height, int width, int channels,\n",
        "                      int num_filters, int kernel_size, int output_height, int output_width) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    int filter_idx = blockIdx.y;\n",
        "    int out_y = threadIdx.x;\n",
        "    int out_x = threadIdx.y;\n",
        "    if (out_y < output_height && out_x < output_width) {\n",
        "        float sum = 0.0f;\n",
        "\n",
        "        for (int c = 0; c < channels; c++) {\n",
        "            for (int ky = 0; ky < kernel_size; ky++) {\n",
        "                for (int kx = 0; kx < kernel_size; kx++) {\n",
        "                    int in_y = out_y + ky;\n",
        "                    int in_x = out_x + kx;\n",
        "\n",
        "                    if (in_y < height && in_x < width) {\n",
        "                        int input_idx = batch_idx * (height * width * channels) +\n",
        "                                      c * (height * width) +\n",
        "                                      in_y * width + in_x;\n",
        "                        int filter_idx_full = filter_idx * (channels * kernel_size * kernel_size) +\n",
        "                                            c * (kernel_size * kernel_size) +\n",
        "                                           ky * kernel_size + kx;\n",
        "\n",
        "                        sum += input[input_idx] * filters[filter_idx_full];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        int output_idx = batch_idx * (num_filters * output_height * output_width) +\n",
        "                        filter_idx * (output_height * output_width) +\n",
        "                        out_y * output_width + out_x;\n",
        "        output[output_idx] = fmaxf(0.0f, sum); // ReLU activation\n",
        "    }\n",
        "}\n",
        "__global__ void max_pool2d(float *input, float *output,\n",
        "                          int batch_size, int height, int width, int channels,\n",
        "                          int pool_size, int output_height, int output_width) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    int channel = blockIdx.y;\n",
        "    int out_y = threadIdx.x;\n",
        "    int out_x = threadIdx.y;\n",
        "\n",
        "    if (out_y < output_height && out_x < output_width) {\n",
        "        float max_val = -1e10f;\n",
        "\n",
        "        for (int py = 0; py < pool_size; py++) {\n",
        "            for (int px = 0; px < pool_size; px++) {\n",
        "                int in_y = out_y * pool_size + py;\n",
        "                int in_x = out_x * pool_size + px;\n",
        "\n",
        "                if (in_y < height && in_x < width) {\n",
        "                    int input_idx = batch_idx * (channels * height * width) +\n",
        "                                  channel * (height * width) +\n",
        "                                  in_y * width + in_x;\n",
        "                    max_val = fmaxf(max_val, input[input_idx]);\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        int output_idx = batch_idx * (channels * output_height * output_width) +\n",
        "                        channel * (output_height * output_width) +\n",
        "                        out_y * output_width + out_x;\n",
        "        output[output_idx] = max_val;\n",
        "        }\n",
        "    }\n",
        "__global__ void fc_forward(float *input, float *weights, float *output,\n",
        "                          int batch_size, int input_size, int output_size) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    int neuron_idx = threadIdx.x;\n",
        "\n",
        "    if (neuron_idx < output_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < input_size; i++) {\n",
        "            sum += input[batch_idx * input_size + i] * weights[i * output_size + neuron_idx];\n",
        "        }\n",
        "        output[batch_idx * output_size + neuron_idx] = sum;\n",
        "    }\n",
        "}\n",
        "__global__ void conv2d_backward(float *d_output, float *filters, float *d_input,\n",
        "                               int batch_size, int height, int width, int channels,\n",
        "                               int num_filters, int kernel_size, int output_height, int output_width) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    int channel = blockIdx.y;\n",
        "    int in_y = threadIdx.x;\n",
        "    int in_x = threadIdx.y;\n",
        "\n",
        "    if (in_y < height && in_x < width) {\n",
        "        float sum = 0.0f;\n",
        "\n",
        "        for (int f = 0; f < num_filters; f++) {\n",
        "            for (int ky = 0; ky < kernel_size; ky++) {\n",
        "                for (int kx = 0; kx < kernel_size; kx++) {\n",
        "                    int out_y = in_y - ky;\n",
        "                    int out_x = in_x - kx;\n",
        "\n",
        "                    if (out_y >= 0 && out_y < output_height && out_x >= 0 && out_x < output_width) {\n",
        "                        int d_output_idx = batch_idx * (num_filters * output_height * output_width) +\n",
        "                                         f * (output_height * output_width) +\n",
        "                                         out_y * output_width + out_x;\n",
        "                        int filter_idx = f * (channels * kernel_size * kernel_size) +\n",
        "                                       channel * (kernel_size * kernel_size) +\n",
        "                                       ky * kernel_size + kx;\n",
        "\n",
        "                        sum += d_output[d_output_idx] * filters[filter_idx];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        int d_input_idx = batch_idx * (channels * height * width) +\n",
        "                         channel * (height * width) +\n",
        "                         in_y * width + in_x;\n",
        "        d_input[d_input_idx] = sum;\n",
        "    }\n",
        "}\n",
        "__global__ void conv2d_update_weights(float *input, float *d_output, float *d_weights,\n",
        "                                    int batch_size, int height, int width, int channels,\n",
        "                                    int num_filters, int kernel_size, int output_height, int output_width) {\n",
        "    int filter_idx = blockIdx.x;\n",
        "    int ky = threadIdx.x;\n",
        "    int kx = threadIdx.y;\n",
        "    int c = blockIdx.y;\n",
        "\n",
        "    if (ky < kernel_size && kx < kernel_size) {\n",
        "        float sum = 0.0f;\n",
        "\n",
        "        for (int b = 0; b < batch_size; b++) {\n",
        "            for (int out_y = 0; out_y < output_height; out_y++) {\n",
        "                for (int out_x = 0; out_x < output_width; out_x++) {\n",
        "                    int in_y = out_y + ky;\n",
        "                    int in_x = out_x + kx;\n",
        "\n",
        "                    if (in_y < height && in_x < width) {\n",
        "                        int input_idx = b * (channels * height * width) +\n",
        "                                      c * (height * width) +\n",
        "                                      in_y * width + in_x;\n",
        "                        int d_output_idx = b * (num_filters * output_height * output_width) +\n",
        "                                         filter_idx * (output_height * output_width) +\n",
        "                                         out_y * output_width + out_x;\n",
        "\n",
        "                        sum += input[input_idx] * d_output[d_output_idx];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "        }\n",
        "\n",
        "        int weight_idx = filter_idx * (channels * kernel_size * kernel_size) +\n",
        "                        c * (kernel_size * kernel_size) +\n",
        "                        ky * kernel_size + kx;\n",
        "        d_weights[weight_idx] = sum / batch_size;\n",
        "    }\n",
        "}\n",
        "__global__ void max_pool_backward(float *input, float *d_output, float *d_input,\n",
        "                                int batch_size, int height, int width, int channels,\n",
        "                                int pool_size, int output_height, int output_width) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    int channel = blockIdx.y;\n",
        "    int in_y = threadIdx.x;\n",
        "    int in_x = threadIdx.y;\n",
        "\n",
        "    if (in_y < height && in_x < width) {\n",
        "        int out_y = in_y / pool_size;\n",
        "        int out_x = in_x / pool_size;\n",
        "\n",
        "        if (out_y < output_height && out_x < output_width) {\n",
        "            int input_idx = batch_idx * (channels * height * width) +\n",
        "                           channel * (height * width) +\n",
        "                           in_y * width + in_x;\n",
        "            int output_idx = batch_idx * (channels * output_height * output_width) +\n",
        "                           channel * (output_height * output_width) +\n",
        "                           out_y * output_width + out_x;\n",
        "           float input_val = input[input_idx];\n",
        "            float max_val = -1e10f;\n",
        "            int max_idx_y = -1, max_idx_x = -1;\n",
        "\n",
        "            for (int py = 0; py < pool_size; py++) {\n",
        "                for (int px = 0; px < pool_size; px++) {\n",
        "                    int curr_y = out_y * pool_size + py;\n",
        "                    int curr_x = out_x * pool_size + px;\n",
        "\n",
        "                    if (curr_y < height && curr_x < width) {\n",
        "                        int curr_idx = batch_idx * (channels * height * width) +\n",
        "                                     channel * (height * width) +\n",
        "                                     curr_y * width + curr_x;\n",
        "                        float curr_val = input[curr_idx];\n",
        "\n",
        "                        if (curr_val > max_val) {\n",
        "                            max_val = curr_val;\n",
        "                            max_idx_y = curr_y;\n",
        "                            max_idx_x = curr_x;\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "\n",
        "            if (in_y == max_idx_y && in_x == max_idx_x) {\n",
        "                d_input[input_idx] = d_output[output_idx];\n",
        "            } else {\n",
        "                d_input[input_idx] = 0.0f;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "__global__ void fc_backward(float *d_output, float *weights, float *d_input,\n",
        "                           int batch_size, int input_size, int output_size) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    int input_idx = threadIdx.x;\n",
        "\n",
        "    if (input_idx < input_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int j = 0; j < output_size; j++) {\n",
        "            sum += d_output[batch_idx * output_size + j] * weights[input_idx * output_size + j];\n",
        "        }\n",
        "        d_input[batch_idx * input_size + input_idx] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void fc_update_weights(float *input, float *d_output, float *d_weights,\n",
        "                                int batch_size, int input_size, int output_size) {\n",
        "    int in_idx = blockIdx.x;\n",
        "    int out_idx = threadIdx.x;\n",
        "\n",
        "    if (in_idx < input_size && out_idx < output_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int b = 0; b < batch_size; b++) {\n",
        "            sum += input[b * input_size + in_idx] * d_output[b * output_size + out_idx];\n",
        "        }\n",
        "        d_weights[in_idx * output_size + out_idx] = sum / batch_size;\n",
        "    }\n",
        "}\n",
        "__global__ void relu(float *x, int n) {\n",
        "            int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "            if (idx < n) {\n",
        "                x[idx] = max(0.0, x[idx]);\n",
        "            }\n",
        "    }\n",
        "__global__ void softmax(float *x, int batch_size, int num_classes) {\n",
        "            int batch_idx = blockIdx.x;\n",
        "            int class_idx = threadIdx.x;\n",
        "\n",
        "            int idx = batch_idx * num_classes + class_idx;\n",
        "\n",
        "            float max_val = -1e9;\n",
        "            for (int j = 0; j < num_classes; j++) {\n",
        "                max_val = fmaxf(max_val, x[batch_idx * num_classes + j]);\n",
        "            }\n",
        "\n",
        "            float sum_exp = 0.0;\n",
        "            for (int j = 0; j < num_classes; j++) {\n",
        "                int current_idx = batch_idx * num_classes + j;\n",
        "                x[current_idx] = expf(x[current_idx] - max_val);\n",
        "                sum_exp += x[current_idx];\n",
        "            }\n",
        "\n",
        "            x[idx] = x[idx] / sum_exp;\n",
        "        }\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "YWnQAnOq7Khf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.conv1_filters = 32\n",
        "        self.conv1_size = 3\n",
        "        self.pool_size = 2\n",
        "        self.fc1_size = 10\n",
        "        self.num_classes = 3\n",
        "\n",
        "        self.input_size = 5\n",
        "        self.conv1_output_size = self.input_size - self.conv1_size + 1\n",
        "        self.pool1_output_size = self.conv1_output_size // self.pool_size\n",
        "\n",
        "        conv1_weights = np.random.randn(self.conv1_filters, 1,\n",
        "                                      self.conv1_size, self.conv1_size).astype(np.float32) * 0.1\n",
        "        self.conv1_weights_gpu = gpuarray.to_gpu(conv1_weights)\n",
        "\n",
        "        fc1_input_size = self.conv1_filters * self.pool1_output_size * self.pool1_output_size\n",
        "        fc1_weights = np.random.randn(fc1_input_size, self.fc1_size).astype(np.float32) * 0.1\n",
        "        self.fc1_weights_gpu = gpuarray.to_gpu(fc1_weights)\n",
        "\n",
        "        fc2_weights = np.random.randn(self.fc1_size, self.num_classes).astype(np.float32) * 0.1\n",
        "        self.fc2_weights_gpu = gpuarray.to_gpu(fc2_weights)\n",
        "\n",
        "        self.max_pool = mod.get_function(\"max_pool2d\")\n",
        "        self.fc_forward = mod.get_function(\"fc_forward\")\n",
        "        self.conv_forward = mod.get_function(\"conv2d\")\n",
        "        self.conv_backward = mod.get_function(\"conv2d_backward\")\n",
        "        self.conv_update_weights = mod.get_function(\"conv2d_update_weights\")\n",
        "        self.pool_backward = mod.get_function(\"max_pool_backward\")\n",
        "        self.fc_backward = mod.get_function(\"fc_backward\")\n",
        "        self.fc_update_weights = mod.get_function(\"fc_update_weights\")\n",
        "        self.relu = mod.get_function(\"relu\")\n",
        "        self.softmax = mod.get_function(\"softmax\")\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.eval_accuracies = []\n",
        "\n",
        "    def backward(self, x_batch, y_batch, output):\n",
        "        batch_size = len(x_batch)\n",
        "\n",
        "        d_output = output.copy()\n",
        "        d_output[range(batch_size), y_batch] -= 1\n",
        "        d_output /= batch_size\n",
        "\n",
        "        d_fc2_output_gpu = gpuarray.to_gpu(d_output)\n",
        "        d_fc1_output = gpuarray.zeros((batch_size, self.fc1_size), dtype=np.float32)\n",
        "\n",
        "        block_dim = (self.fc1_size, 1, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        self.fc_backward(d_fc2_output_gpu, self.fc2_weights_gpu, d_fc1_output,\n",
        "                        np.int32(batch_size), np.int32(self.fc1_size), np.int32(self.num_classes),\n",
        "                        block=block_dim, grid=grid_dim)\n",
        "\n",
        "        d_fc2_weights = gpuarray.zeros_like(self.fc2_weights_gpu)\n",
        "\n",
        "        block_dim = (self.num_classes, 1, 1)\n",
        "        grid_dim = (self.fc1_size, 1, 1)\n",
        "\n",
        "        self.fc_update_weights(d_fc1_output, d_fc2_output_gpu, d_fc2_weights,\n",
        "                             np.int32(batch_size), np.int32(self.fc1_size), np.int32(self.num_classes),\n",
        "                             block=block_dim, grid=grid_dim)\n",
        "\n",
        "        d_pool1_output = gpuarray.zeros((batch_size, self.conv1_filters,\n",
        "                                       self.pool1_output_size, self.pool1_output_size),\n",
        "                                      dtype=np.float32)\n",
        "\n",
        "        flattened_size = self.conv1_filters * self.pool1_output_size * self.pool1_output_size\n",
        "\n",
        "        block_dim = (flattened_size, 1, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        self.fc_backward(d_fc1_output, self.fc1_weights_gpu, d_pool1_output,\n",
        "                        np.int32(batch_size), np.int32(flattened_size), np.int32(self.fc1_size),\n",
        "                        block=block_dim, grid=grid_dim)\n",
        "        d_fc1_weights = gpuarray.zeros_like(self.fc1_weights_gpu)\n",
        "\n",
        "        block_dim = (self.fc1_size, 1, 1)\n",
        "        grid_dim = (flattened_size, 1, 1)\n",
        "\n",
        "        self.fc_update_weights(d_pool1_output, d_fc1_output, d_fc1_weights,\n",
        "                             np.int32(batch_size), np.int32(flattened_size), np.int32(self.fc1_size),\n",
        "                             block=block_dim, grid=grid_dim)\n",
        "\n",
        "        d_conv1_output = gpuarray.zeros((batch_size, self.conv1_filters,\n",
        "                                       self.conv1_output_size, self.conv1_output_size),\n",
        "                                      dtype=np.float32)\n",
        "\n",
        "        block_dim = (self.conv1_output_size, self.conv1_output_size, 1)\n",
        "        grid_dim = (batch_size, self.conv1_filters, 1)\n",
        "\n",
        "        self.pool_backward(d_conv1_output, d_pool1_output, d_conv1_output,\n",
        "                          np.int32(batch_size), np.int32(self.conv1_output_size),\n",
        "                          np.int32(self.conv1_output_size), np.int32(self.conv1_filters),\n",
        "                          np.int32(self.pool_size), np.int32(self.pool1_output_size),\n",
        "                          np.int32(self.pool1_output_size),\n",
        "                          block=block_dim, grid=grid_dim)\n",
        "\n",
        "        d_input = gpuarray.zeros((batch_size, 1, 5, 5), dtype=np.float32)\n",
        "\n",
        "        block_dim = (5, 5, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        self.conv_backward(d_conv1_output, self.conv1_weights_gpu, d_input,\n",
        "                          np.int32(batch_size), np.int32(5), np.int32(5), np.int32(1),\n",
        "                          np.int32(self.conv1_filters), np.int32(self.conv1_size),\n",
        "                          np.int32(self.conv1_output_size), np.int32(self.conv1_output_size),\n",
        "                          block=block_dim, grid=grid_dim)\n",
        "\n",
        "        d_conv1_weights = gpuarray.zeros_like(self.conv1_weights_gpu)\n",
        "\n",
        "        block_dim = (self.conv1_size, self.conv1_size, 1)\n",
        "        grid_dim = (self.conv1_filters, 1, 1)\n",
        "\n",
        "        self.conv_update_weights(d_input, d_conv1_output, d_conv1_weights,\n",
        "                               np.int32(batch_size), np.int32(5), np.int32(5), np.int32(1),\n",
        "                               np.int32(self.conv1_filters), np.int32(self.conv1_size),\n",
        "                               np.int32(self.conv1_output_size), np.int32(self.conv1_output_size),\n",
        "                               block=block_dim, grid=grid_dim)\n",
        "\n",
        "        self.conv1_weights_gpu -= self.learning_rate * d_conv1_weights\n",
        "        self.fc1_weights_gpu -= self.learning_rate * d_fc1_weights\n",
        "        self.fc2_weights_gpu -= self.learning_rate * d_fc2_weights\n",
        "\n",
        "        d_fc2_output_gpu.gpudata.free()\n",
        "        d_fc1_output.gpudata.free()\n",
        "        d_pool1_output.gpudata.free()\n",
        "        d_conv1_output.gpudata.free()\n",
        "        d_input.gpudata.free()\n",
        "        d_conv1_weights.gpudata.free()\n",
        "        d_fc1_weights.gpudata.free()\n",
        "        d_fc2_weights.gpudata.free()\n",
        "\n",
        "\n",
        "    def compute_loss(self, output, y_batch):\n",
        "        \"\"\"Compute categorical cross-entropy loss.\"\"\"\n",
        "        batch_size = len(y_batch)\n",
        "        output_cpu = output.get()\n",
        "        log_likelihood = -np.log(output_cpu[range(batch_size), y_batch])\n",
        "        loss = np.sum(log_likelihood) / batch_size\n",
        "        return loss\n",
        "\n",
        "    def compute_accuracy(self, output, y_batch):\n",
        "        \"\"\"Compute classification accuracy.\"\"\"\n",
        "        predictions = np.argmax(output.get(), axis=1)\n",
        "        return np.mean(predictions == y_batch)\n",
        "\n",
        "    def forward(self, x_batch):\n",
        "        \"\"\"Forward pass returning output probabilities.\"\"\"\n",
        "        batch_size = len(x_batch)\n",
        "\n",
        "        input_gpu = gpuarray.to_gpu(x_batch.astype(np.float32))\n",
        "\n",
        "        # Convolution layer\n",
        "        conv1_output = gpuarray.zeros((batch_size, self.conv1_filters,\n",
        "                                     self.conv1_output_size, self.conv1_output_size),\n",
        "                                    dtype=np.float32)\n",
        "\n",
        "        block_dim = (self.conv1_output_size, self.conv1_output_size, 1)\n",
        "        grid_dim = (batch_size, self.conv1_filters, 1)\n",
        "\n",
        "        self.conv_forward(input_gpu, self.conv1_weights_gpu, conv1_output,\n",
        "                         np.int32(batch_size), np.int32(5), np.int32(5), np.int32(1),\n",
        "                         np.int32(self.conv1_filters), np.int32(self.conv1_size),\n",
        "                         np.int32(self.conv1_output_size), np.int32(self.conv1_output_size),\n",
        "                         block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # Max pooling layer\n",
        "        pool1_output = gpuarray.zeros((batch_size, self.conv1_filters,\n",
        "                                     self.pool1_output_size, self.pool1_output_size),\n",
        "                                    dtype=np.float32)\n",
        "\n",
        "        block_dim = (self.pool1_output_size, self.pool1_output_size, 1)\n",
        "        grid_dim = (batch_size, self.conv1_filters, 1)\n",
        "\n",
        "        self.max_pool(conv1_output, pool1_output,\n",
        "                     np.int32(batch_size), np.int32(self.conv1_output_size),\n",
        "                     np.int32(self.conv1_output_size), np.int32(self.conv1_filters),\n",
        "                     np.int32(self.pool_size), np.int32(self.pool1_output_size),\n",
        "                     np.int32(self.pool1_output_size),\n",
        "                     block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # Flatten pooling output\n",
        "        flattened = pool1_output.reshape(batch_size, -1)\n",
        "\n",
        "        # First fully connected layer\n",
        "        fc1_output = gpuarray.zeros((batch_size, self.fc1_size), dtype=np.float32)\n",
        "\n",
        "        block_dim = (self.fc1_size, 1, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        self.fc_forward(flattened, self.fc1_weights_gpu, fc1_output,\n",
        "                       np.int32(batch_size), np.int32(flattened.shape[1]),\n",
        "                       np.int32(self.fc1_size),\n",
        "                       block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # ReLU activation\n",
        "        self.relu(fc1_output, np.int32(fc1_output.size),\n",
        "                 block=(256, 1, 1), grid=((fc1_output.size + 255) // 256, 1, 1))\n",
        "\n",
        "        # Second fully connected layer (output layer)\n",
        "        output = gpuarray.zeros((batch_size, self.num_classes), dtype=np.float32)\n",
        "\n",
        "        block_dim = (self.num_classes, 1, 1)\n",
        "        grid_dim = (batch_size, 1, 1)\n",
        "\n",
        "        self.fc_forward(fc1_output, self.fc2_weights_gpu, output,\n",
        "                       np.int32(batch_size), np.int32(self.fc1_size),\n",
        "                       np.int32(self.num_classes),\n",
        "                       block=block_dim, grid=grid_dim)\n",
        "\n",
        "        # Softmax activation\n",
        "        self.softmax(output, np.int32(batch_size), np.int32(self.num_classes),\n",
        "                    block=(self.num_classes, 1, 1), grid=(batch_size, 1, 1))\n",
        "\n",
        "        return output, conv1_output, pool1_output, fc1_output\n",
        "\n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, epochs=10, batch_size=32):\n",
        "        \"\"\"Train the CNN using mini-batch gradient descent.\"\"\"\n",
        "        n_samples = len(X_train)\n",
        "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_train = X_train[indices]\n",
        "            y_train = y_train[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "            epoch_accuracy = 0\n",
        "\n",
        "            for batch in range(n_batches):\n",
        "                start_idx = batch * batch_size\n",
        "                end_idx = min((batch + 1) * batch_size, n_samples)\n",
        "\n",
        "                x_batch = X_train[start_idx:end_idx]\n",
        "                y_batch = y_train[start_idx:end_idx]\n",
        "\n",
        "                output, conv1_output, pool1_output, fc1_output = self.forward(x_batch)\n",
        "\n",
        "                batch_loss = self.compute_loss(output, y_batch)\n",
        "                batch_accuracy = self.compute_accuracy(output, y_batch)\n",
        "\n",
        "                epoch_loss += batch_loss * len(x_batch)\n",
        "                epoch_accuracy += batch_accuracy * len(x_batch)\n",
        "\n",
        "                self.backward(x_batch, y_batch, output.get())\n",
        "\n",
        "                conv1_output.gpudata.free()\n",
        "                pool1_output.gpudata.free()\n",
        "                fc1_output.gpudata.free()\n",
        "                output.gpudata.free()\n",
        "\n",
        "            epoch_loss /= n_samples\n",
        "            epoch_accuracy /= n_samples\n",
        "\n",
        "            self.train_losses.append(epoch_loss)\n",
        "            self.train_accuracies.append(epoch_accuracy)\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "    def evaluate(self, X_test, y_test, batch_size=32):\n",
        "        \"\"\"Evaluate the CNN on test data.\"\"\"\n",
        "        n_samples = len(X_test)\n",
        "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
        "        total_accuracy = 0\n",
        "\n",
        "        for batch in range(n_batches):\n",
        "            start_idx = batch * batch_size\n",
        "            end_idx = min((batch + 1) * batch_size, n_samples)\n",
        "\n",
        "            x_batch = X_test[start_idx:end_idx]\n",
        "            y_batch = y_test[start_idx:end_idx]\n",
        "\n",
        "            output, conv1_output, pool1_output, fc1_output = self.forward(x_batch)\n",
        "\n",
        "            batch_accuracy = self.compute_accuracy(output, y_batch)\n",
        "            total_accuracy += batch_accuracy * len(x_batch)\n",
        "\n",
        "            conv1_output.gpudata.free()\n",
        "            pool1_output.gpudata.free()\n",
        "            fc1_output.gpudata.free()\n",
        "            output.gpudata.free()\n",
        "\n",
        "        return total_accuracy / n_samples\n",
        "\n",
        "    def predict(self, X, batch_size=32):\n",
        "        \"\"\"Generate predictions for input data.\"\"\"\n",
        "        n_samples = len(X)\n",
        "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
        "        predictions = np.zeros(n_samples, dtype=np.int32)\n",
        "\n",
        "        for batch in range(n_batches):\n",
        "            start_idx = batch * batch_size\n",
        "            end_idx = min((batch + 1) * batch_size, n_samples)\n",
        "\n",
        "            x_batch = X[start_idx:end_idx]\n",
        "\n",
        "            output, conv1_output, pool1_output, fc1_output = self.forward(x_batch)\n",
        "\n",
        "            batch_predictions = np.argmax(output.get(), axis=1)\n",
        "            predictions[start_idx:end_idx] = batch_predictions\n",
        "\n",
        "            # Free GPU memory\n",
        "            conv1_output.gpudata.free()\n",
        "            pool1_output.gpudata.free()\n",
        "            fc1_output.gpudata.free()\n",
        "            output.gpudata.free()\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "mI3Cwy__7RHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = CNN(learning_rate=1e-4)\n",
        "\n",
        "cnn.train(X_train, y_train, X_test, y_test, epochs=10, batch_size=21)\n",
        "test_accuracy = cnn.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy : {test_accuracy}\")"
      ],
      "metadata": {
        "id": "jWgz_GRj7XaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402510e9-ba4c-4b8f-e317-230a14a77639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 1.0989 - Accuracy: 0.2857\n",
            "Epoch 2/10 - Loss: 1.0989 - Accuracy: 0.2857\n",
            "Epoch 3/10 - Loss: 1.0989 - Accuracy: 0.2857\n",
            "Epoch 4/10 - Loss: 1.0989 - Accuracy: 0.2857\n",
            "Epoch 5/10 - Loss: 1.0989 - Accuracy: 0.2857\n",
            "Epoch 6/10 - Loss: 1.0989 - Accuracy: 0.2857\n",
            "Epoch 7/10 - Loss: 1.0989 - Accuracy: 0.2857\n",
            "Epoch 8/10 - Loss: 1.0989 - Accuracy: 0.2857\n",
            "Epoch 9/10 - Loss: 1.0989 - Accuracy: 0.2857\n",
            "Epoch 10/10 - Loss: 1.0989 - Accuracy: 0.2857\n",
            "Test Accuracy : 0.5555555555555556\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: device_allocation in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        }
      ]
    }
  ]
}